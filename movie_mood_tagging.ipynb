{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mood Metatagging Script\n",
    "\n",
    "A useful script to extract metatags for any given movie input. The script identifies all adjectives from blocks of text in scraped html files and employs an iterative synonym matching process based on manually coded word vectors. The html files in question are stored on Google Cloud Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcp_access\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_trf\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mood_tag:\n",
    "    \n",
    "    def __init__(self, imdb_id, source, coded_word_list, irrelevant_word_list):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - imdb_id: a list of imdb id's in string format\n",
    "        - source: a list of web sources to extract tags from, such as 'wikipedia' or 'fandom'\n",
    "        - coded_word_list: manually coded word list in csv format, with the second column named 'keyword'\n",
    "        - irrelevant_word_list: a list of words excluded from the tagging process\n",
    "        \"\"\"\n",
    "        # get keyword, mood, and genes from coded_word_list\n",
    "        self.df = pd.read_csv(coded_word_list, header=0, index_col=0)\n",
    "        self.keyword = self.df[self.df['keyword']==1]['keyword'].index.tolist()\n",
    "        self.mood = sorted(self.df[self.df['keyword']==0]['keyword'].index.tolist())\n",
    "        #self.gene = self.df.columns[self.df.columns.difference(['keyword'])].tolist()\n",
    "        \n",
    "        # get a dictionary of pretrained vectors\n",
    "        self.pretrained_vectors = {word: nlp(word) for word in self.mood}\n",
    "\n",
    "        # get list of irrelevant words\n",
    "        self.irrelevant_words = pd.read_csv(irrelevant_word_list, header=None)[0].tolist()\n",
    "\n",
    "        # initialize imdb_id and source\n",
    "        self.imdb_id = imdb_id\n",
    "        self.source = source\n",
    "\n",
    "        # initialize similarity matrix\n",
    "        self.sim = self.build_sim()\n",
    "        \n",
    "        # initialize gcp_access\n",
    "        self.gcp = gcp_access.gcp()\n",
    "\n",
    "    def build_sim(self):\n",
    "        \"\"\"Build a cosine similarity matrix based on coded word vectors\"\"\"\n",
    "        mat = self.df[self.df.columns.difference(['keyword'])].values\n",
    "        sim = pd.DataFrame(1 - pairwise_distances(mat, metric=\"cosine\"))\n",
    "        sim.columns = self.df.index[~self.df.index.isin(['keyword'])]\n",
    "        sim.index = sim.columns\n",
    "        # print(sim)        \n",
    "        return sim\n",
    "\n",
    "\n",
    "    def process_text(self, imdb_id, filepath):    \n",
    "        \"\"\"Download and process text from GCP\"\"\"\n",
    "        try:\n",
    "            html = ' '.join(self.gcp.read_html_by_filepath(filepath))\n",
    "            # print(html)\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            text = ' '.join([content.text for content in soup.findAll('p')])\n",
    "            # print(text)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "        # extract adj\n",
    "        tokens = nlp(text)\n",
    "        adj = [token.text.lower() for token in tokens if token.pos_ == 'ADJ' and token.text.lower() not in self.irrelevant_words]\n",
    "        # print(adj)\n",
    "        \n",
    "        # search for genre \"key\" words\n",
    "        noun = re.findall('|'.join(self.keyword), text.lower())\n",
    "\n",
    "        # return word list\n",
    "        word_list = list(set(adj + noun))\n",
    "        if len(word_list) > 0:\n",
    "            return word_list\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def extract_adj(self, output_filepath):\n",
    "        \"\"\"Find final list of adj for each movie and write to a csv file\"\"\"\n",
    "        df = self.gcp.read_master_csv()\n",
    "        df = df.loc[(df['imdb_id'].isin(imdb_id)) & (df['source'].isin(['fandom', 'wikipedia'])), ['imdb_id', 'filepath']]\n",
    "        # print(df)\n",
    "        imdb = df['imdb_id'].unique().tolist()\n",
    "        for i in range(len(imdb)):\n",
    "            filepath = df[df['imdb_id']==imdb[i]]['filepath'].tolist()\n",
    "            adj = self.process_text(imdb[i], filepath)\n",
    "            if adj:\n",
    "                nested_list = [self.find_knn(word) for word in adj]      \n",
    "                final_list = list(set([j for i in nested_list for j in i]))\n",
    "                # print(final_list)\n",
    "                with open(output_filepath, \"a\") as file:\n",
    "                    file.write(imdb[i] + \",\" + '|'.join(final_list) +\"\\n\")\n",
    "            else:\n",
    "                print('no adj for ' + imdb[i])\n",
    "\n",
    "\n",
    "    def find_knn(self, word):\n",
    "        \"\"\"Find relevant synonyms based on manually coded word vectors and the cosine similarity matrx\"\"\"\n",
    "        \n",
    "        # keep record of a list of new words (words that haven't been manually coded) and their synonym mappings\n",
    "        global new_word\n",
    "\n",
    "        try:\n",
    "            matches = self.sim.loc[self.sim[word]==1, word].index.tolist()\n",
    "            if len(matches) < 10:\n",
    "                # take neighbors with cosine similarity >= threshold\n",
    "                synonyms = self.sim.loc[self.sim[word] >= 0.50, word].index.tolist()\n",
    "                return synonyms\n",
    "            return matches\n",
    "\n",
    "        except:\n",
    "            # synonym matching of unknown word\n",
    "            vector = nlp(word)\n",
    "            similarities = [vector.similarity(self.pretrained_vectors[vectors]) for vectors in self.mood]\n",
    "            # print(similarities)\n",
    "            top_sim_value = sorted(similarities)[-1]\n",
    "            top_sim_index = similarities.index(top_sim_value)\n",
    "            top_sim = self.mood[top_sim_index]\n",
    "            # record our customized one-hot encoding for new vocab \n",
    "            if top_sim_value > 4.0:\n",
    "                new_word[word] = self.df.loc[top_sim].values.tolist()\n",
    "                return [word] + find_knn(top_sim)\n",
    "            else:\n",
    "                return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # create a dictionary called new_word\n",
    "    new_word = dict()\n",
    "    \n",
    "    # specify input parameters\n",
    "    imdb_id = None\n",
    "    source = ['fandom', 'wikipedia']\n",
    "    output_filepath = None\n",
    "    \n",
    "    # instantiate a mood_tag object\n",
    "    a = mood_tag(imdb_id, source)\n",
    "    \n",
    "    # write mood tags for each movie to a csv file\n",
    "    a.extract_adj(output_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
